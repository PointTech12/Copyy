

## ğŸ“ Project Structure

```
Copyy/
â”œâ”€â”€ main.py                          # Main processing pipeline
â”œâ”€â”€ main_2.py                        # Metadata extraction utility
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ book_metadata.json              # Book metadata storage
â”œâ”€â”€ Dataset_PDFs/                   # Input PDF files
â”œâ”€â”€ Processed_Data/                 # Output processed data
â”‚   â”œâ”€â”€ all_processed_books.csv     # Combined analysis results
â”œâ”€â”€ New_Dataset/                    # New dataset processing
â”‚   â””â”€â”€ metadata.json              # Extracted metadata
```

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd Copyy
   ```

2. **Create a virtual environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Install spaCy language model**:
   ```bash
   python -m spacy download en_core_web_sm
   ```

5. **Install additional system dependencies** (for OCR functionality):
   ```bash
   sudo apt-get install tesseract-ocr
   sudo apt-get install poppler-utils

   ```

## ğŸ“– Usage

### Basic Processing

1. **Place your PDF files** in the `Dataset_PDFs/` directory

2. **Run the main processing pipeline**:
   ```bash
   python main.py
   ```

3. **Extract metadata** (optional):
   ```bash
   python main_2.py
   ```


## ğŸ”§ Configuration

### Main Processing Settings

The main processing pipeline can be configured by modifying the paths in `main.py`:

```python
dataset_path = "/path/to/your/pdf/files"
output_path = "/path/to/output/directory"
```

---


